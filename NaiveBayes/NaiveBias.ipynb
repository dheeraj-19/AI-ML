{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "People’s name can be connected to which country he/she comes from. Here we have 4000 (fake) names: Japanese, American, Arabic, and Greek. Implement a NB classifier that can make a prediction given a new\n",
        "name.\n",
        "\n"
      ],
      "metadata": {
        "id": "lTe46zzyAU5F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8KdSRfUmfnk",
        "outputId": "da811c52-d41c-4150-dff4-2f7d63872dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Reading the data\n",
        "import os\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "datapath = '/content/drive/My Drive/Colab Notebooks/'\n",
        "os.path.exists(datapath)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging all the names in one CSV File\n",
        "\n",
        "**File Name:-** names.csv\n",
        "\n",
        "**Content:-**\n",
        "\n",
        "  *Header:* Name, Country\n",
        "\n",
        "  *Data:* Name from each txt file, Country to which the name belongs (American, Arabic, Greek, and Japanese)"
      ],
      "metadata": {
        "id": "acNsw3xKAju0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(datapath + 'AML_HW3/names.csv'):\n",
        "    os.remove(datapath + 'AML_HW3/names.csv')\n",
        "\n",
        "head = \"Name,Country\\n\"\n",
        "with open(datapath + 'AML_HW3/names.csv', 'a') as csv_file:\n",
        "  csv_file.write(head)\n",
        "\n",
        "  with open(datapath + \"AML_HW3/us.txt\") as txt_file:\n",
        "      line = txt_file.readline()\n",
        "      while(line):\n",
        "        txt = line.strip() + \",American\\n\"\n",
        "        csv_file.write(txt)\n",
        "        line = txt_file.readline()\n",
        "\n",
        "  with open(datapath + \"AML_HW3/arabic.txt\") as txt_file:\n",
        "      line = txt_file.readline()\n",
        "      while(line):\n",
        "        txt = line.strip() + \",Arabic\\n\"\n",
        "        csv_file.write(txt)\n",
        "        line = txt_file.readline()\n",
        "\n",
        "  with open(datapath + \"AML_HW3/greek.txt\") as txt_file:\n",
        "      line = txt_file.readline()\n",
        "      while(line):\n",
        "        txt = line.strip() + \",Greek\\n\"\n",
        "        csv_file.write(txt)\n",
        "        line = txt_file.readline()\n",
        "\n",
        "  with open(datapath + \"AML_HW3/japan.txt\") as txt_file:\n",
        "      line = txt_file.readline()\n",
        "      while(line):\n",
        "        txt = line.strip() + \",Japanese\\n\"\n",
        "        csv_file.write(txt)\n",
        "        line = txt_file.readline()\n"
      ],
      "metadata": {
        "id": "vl5OyA3E3w1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the data to know what we have\n",
        "data = pd.read_csv(datapath + \"AML_HW3/names.csv\")\n",
        "data_copy = data.copy()\n",
        "print(data.describe())"
      ],
      "metadata": {
        "id": "JfgvYEPapDeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e320dda0-4822-478a-c306-c3d5864a3aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Name   Country\n",
            "count   4000      4000\n",
            "unique  3775         4\n",
            "top     鈴木 零  American\n",
            "freq       5      1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using CountVectorizer to vectorize the input names"
      ],
      "metadata": {
        "id": "57668HwbFyi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_vectorizer = CountVectorizer()\n",
        "data_X = name_vectorizer.fit_transform(data.Name)\n",
        "name_vectorizer.get_feature_names_out()\n",
        "\n",
        "print(data_X.toarray())\n",
        "print(data_X.shape)\n",
        "print(len(name_vectorizer.get_feature_names_out()))\n",
        "print(name_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "1SufWj5F74_J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf387cb-00c6-457c-9f0e-e80ebbafcfed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "(4000, 3043)\n",
            "3043\n",
            "['aaron' 'abbott' 'abigail' ... '青木' '香織' '高橋']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the data into training (70%) and testing (30%) with shuffle = True"
      ],
      "metadata": {
        "id": "w7YaWxx_F7Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(data_X, data.Country, test_size=0.3, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "ecT2ln30rPpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X Train: \", x_train.shape, \"Y Train: \", len(y_train))\n",
        "print(\"X Test: \", x_test.shape, \"Y Test: \", len(y_test))"
      ],
      "metadata": {
        "id": "J5t97attryF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24449cda-981b-45c6-fe8e-3eb0390592a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X Train:  (2800, 3043) Y Train:  2800\n",
            "X Test:  (1200, 3043) Y Test:  1200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting sparse matrix to dense matrix for applying Naive Bayes\n",
        "X_train = x_train.toarray()\n",
        "X_test = x_test.toarray()"
      ],
      "metadata": {
        "id": "XQLASYOLsYAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performing a test with sklearn function\n",
        "\n",
        "To check if the data could get predictions\n",
        "\n",
        "To compare score with our implementation"
      ],
      "metadata": {
        "id": "D86OuWWEGQX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Link to MultinomialNB https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "clf.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "VcoWoNbOucWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e67060d6-63ec-4ef1-c821-879de61b8f34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.945"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing the Multinomial Naive Bayes algorithm"
      ],
      "metadata": {
        "id": "J76TV01BHS3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "HiaomnmSIaHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No of training samples\n",
        "names_count = X_train.shape[0]\n",
        "print(names_count)\n",
        "\n",
        "# Divide the training data(names) based on the four countries\n",
        "countries = [[p for p, q in zip(X_train, y_train) if q == r] for r in np.unique(y_train)]\n",
        "print(len(countries))\n",
        "\n",
        "# Calcualte the prior log probabilities for each country\n",
        "countries_prior_log_p = [np.log(len(p) / names_count) for p in countries]\n",
        "print(countries_prior_log_p)\n",
        "\n",
        "# Count each name for each class and add 1 to it as a smoothing parameter\n",
        "count_names_per_country = np.array([np.array(p).sum(axis=0) for p in countries]) + 1\n",
        "print(count_names_per_country)\n",
        "\n",
        "# Calculate log probabilities of each name\n",
        "name_log_p = np.log(count_names_per_country / count_names_per_country.sum(axis=1)[np.newaxis].T)\n",
        "print(name_log_p)"
      ],
      "metadata": {
        "id": "Q9Z6ptpb_PFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "476fe2f8-b60c-4591-93d9-605509159e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2800\n",
            "4\n",
            "[-1.40795585790107, -1.3963446969733921, -1.377759458670053, -1.3636945292026497]\n",
            "[[ 6  2  2 ...  1  1  1]\n",
            " [ 1  1  1 ...  1  1  1]\n",
            " [ 1  1  1 ...  1  1  1]\n",
            " [ 1  1  1 ...  5 22 39]]\n",
            "[[-6.60800063 -7.70661291 -7.70661291 ... -8.39976009 -8.39976009\n",
            "  -8.39976009]\n",
            " [-8.53030683 -8.53030683 -8.53030683 ... -8.53030683 -8.53030683\n",
            "  -8.53030683]\n",
            " [-8.42463921 -8.42463921 -8.42463921 ... -8.42463921 -8.42463921\n",
            "  -8.42463921]\n",
            " [-8.370316   -8.370316   -8.370316   ... -6.76087808 -5.27927354\n",
            "  -4.70675435]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting the model based on the above training"
      ],
      "metadata": {
        "id": "4MDL8ganIdBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the log probabilities of each country from the test set\n",
        "predict_log_p = [(name_log_p * p).sum(axis=1) + countries_prior_log_p for p in X_test]\n",
        "\n",
        "# Pick the country with the max log probability value\n",
        "prediction = np.argmax(predict_log_p, axis=1)"
      ],
      "metadata": {
        "id": "dORPwyOdJbE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction)\n",
        "print(y_test)"
      ],
      "metadata": {
        "id": "g-4lAztMKQ1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "740bce68-8837-4445-cab8-14e34c2ad871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 3 0 ... 1 0 3]\n",
            "555     American\n",
            "3491    Japanese\n",
            "527     American\n",
            "3925    Japanese\n",
            "2989       Greek\n",
            "          ...   \n",
            "3856    Japanese\n",
            "226     American\n",
            "1612      Arabic\n",
            "535     American\n",
            "3848    Japanese\n",
            "Name: Country, Length: 1200, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the score and accuracy"
      ],
      "metadata": {
        "id": "fHsB4puoI7KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "country = {\n",
        "    \"American\" : 0,\n",
        "    \"Arabic\" : 1,\n",
        "    \"Greek\" : 2,\n",
        "    \"Japanese\" : 3\n",
        "}\n",
        "\n",
        "correct = 0\n",
        "\n",
        "for p in range(len(y_test)):\n",
        "  if prediction[p] == country[y_test[p:p+1].tolist()[0].strip()]:\n",
        "    correct += 1\n",
        "\n",
        "score = correct/len(y_test)\n",
        "accuracy = 100 * score\n",
        "print(\"Score: \", score)\n",
        "print(\"Accuracy: \", accuracy) "
      ],
      "metadata": {
        "id": "FX7myvAGLMHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc17541-fd4f-4c06-ee6e-b5cb35c625ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score:  0.945\n",
            "Accuracy:  94.5\n"
          ]
        }
      ]
    }
  ]
}